{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import moments\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "dir = \"/scratch/djb3ve/connor/\"\n",
    "pickle_file = dir + \"run_moments/daphnia.filtered.chr.busco_data_dict.pickle\"\n",
    "#pickle_file = dir + \"run_moments/daphnia.filt.mlg.genome.11.18.22_data_dict.pickle\"\n",
    "output_file = dir + \"sfs_statistics.txt\"\n",
    "csv_dir = dir + \"sfs_csvs/\"\n",
    "sfs_dir = dir + \"sfss/\"\n",
    "pop_ids = [\"Daphnia.pulex.NorthAmerica\", \"Daphnia.pulex.Europe\"]\n",
    "prior_estimated_params = [7e5, 2e5, 1e7 / 2, 1e-8 * 2]\n",
    "split_mig_params = [6.358476, 1.032427, 9.989897, 0.007727318]\n",
    "split_no_mig_params = [1.065988, 0.2213014, 0.8256301, 0]\n",
    "#split_mig_params = [6.758185, 1.1126711, 9.9724427, 0.008836859]\n",
    "#split_no_mig_params = [1.127693, 0.2289919, 0.7780228, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sfs_as_csv_and_npy(sfs, sfs_name):\n",
    "    # Save SFS as CSV\n",
    "    np.savetxt(csv_dir + sfs_name + \".csv\", sfs, delimiter=\",\")\n",
    "    # Save SFS as numpy binary\n",
    "    with open(sfs_dir + sfs_name + \".npy\", \"wb\") as fout:\n",
    "        np.save(fout, sfs.data)\n",
    "\n",
    "def get_AIC(sfs, sfs_empirical, k):\n",
    "    # k is the number of model parameters.\n",
    "    ll = moments.Inference.ll_multinom(sfs, sfs_empirical)\n",
    "    AIC = k * 2 - 2 * ll\n",
    "    return AIC\n",
    "\n",
    "def get_BIC(sfs, sfs_empirical, k, sfs_size):\n",
    "    # k is the number of model parameters.\n",
    "    # The number of elements in a square folded SFS with the absent allele corner\n",
    "    # masked is the ith triangular number minus 1, where i is the sample size.\n",
    "    n = sfs_size * (sfs_size + 1) / 2 - 1\n",
    "    ll = moments.Inference.ll_multinom(sfs, sfs_empirical)\n",
    "    BIC = k * log(n) - 2 * ll\n",
    "    return BIC\n",
    "\n",
    "def get_shared_allele_prop(sfs, maf_threshold=0.01):\n",
    "    # Iterate through sfs, summing element values, but skipping entries corresponding\n",
    "    # to alleles that are not sufficiently shared according to \"maf_threshold\"\n",
    "    shared_allele_count = 0\n",
    "    coordinate_thresholds = np.array([(len - 1) * maf_threshold + 1 for len in sfs.shape]).astype(int)\n",
    "\n",
    "    # Create NumPy iterator object\n",
    "    it = np.nditer(sfs, flags=['multi_index'])\n",
    "    for i in it:\n",
    "        shared_allele = True\n",
    "        # Check for whether element should be skipped because it fails to cross\n",
    "        # MAF threshold\n",
    "        for j, coord in enumerate(it.multi_index):\n",
    "            if coord < coordinate_thresholds[j]:\n",
    "                shared_allele = False\n",
    "                break\n",
    "\n",
    "        # If it crosses the MAF threshold to be considered \"shared\", add it to the counter\n",
    "        if shared_allele:\n",
    "            shared_allele_count += i\n",
    "\n",
    "    shared_allele_prop = shared_allele_count / np.sum(sfs)\n",
    "    return shared_allele_prop\n",
    "\n",
    "def write_output(fout, outputs):\n",
    "    for output in outputs:\n",
    "        fout.write(str(output) + \"\\t\")\n",
    "    fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_file, \"rb\") as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [20, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_empirical = moments.Spectrum.from_data_dict(data_dict, pop_ids=pop_ids,\n",
    "                                                  projections=ns,\n",
    "                                                  polarized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_from_ests = moments.Demographics2D.split_mig(prior_estimated_params, ns,\n",
    "                                                   pop_ids=pop_ids).fold()\n",
    "sfs_split_mig_model = moments.Demographics2D.split_mig(split_mig_params, ns,\n",
    "                                                              pop_ids=pop_ids).fold()\n",
    "sfs_split_no_mig_model = moments.Demographics2D.split_mig(split_no_mig_params, ns,\n",
    "                                                                 pop_ids=pop_ids).fold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.774635228397063"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.Inference.ll_multinom(sfs_from_ests, sfs_empirical / np.sum(sfs_empirical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.489374807789013"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.Inference.ll_multinom(sfs_split_mig_model, sfs_empirical / np.sum(sfs_empirical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.517175603071643"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.Inference.ll_multinom(sfs_split_no_mig_model, sfs_empirical / np.sum(sfs_empirical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = dir + \"run_moments/daphnia.filt.mlg.genome.11.18.22_data_dict.pickle\"\n",
    "split_mig_params = [6.758185, 1.1126711, 9.9724427, 0.008836859]\n",
    "split_no_mig_params = [1.127693, 0.2289919, 0.7780228, 0]\n",
    "with open(pickle_file, \"rb\") as f:\n",
    "    data_dict_filt = pickle.load(f)\n",
    "sfs_empirical_filt = moments.Spectrum.from_data_dict(data_dict_filt, pop_ids=pop_ids,\n",
    "                                                  projections=ns,\n",
    "                                                  polarized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_from_ests_filt = moments.Demographics2D.split_mig(prior_estimated_params, ns,\n",
    "                                                   pop_ids=pop_ids).fold()\n",
    "sfs_split_mig_model_filt = moments.Demographics2D.split_mig(split_mig_params, ns,\n",
    "                                                              pop_ids=pop_ids).fold()\n",
    "sfs_split_no_mig_model_filt = moments.Demographics2D.split_mig(split_no_mig_params, ns,\n",
    "                                                                 pop_ids=pop_ids).fold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.8305639089964485"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.Inference.ll_multinom(sfs_from_ests_filt, sfs_empirical_filt / np.sum(sfs_empirical_filt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.513468319173573"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.Inference.ll_multinom(sfs_split_mig_model_filt, sfs_empirical_filt / np.sum(sfs_empirical_filt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.5417243695695"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.Inference.ll_multinom(sfs_split_no_mig_model_filt, sfs_empirical_filt / np.sum(sfs_empirical_filt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_empirical = moments.Spectrum.from_data_dict(data_dict, pop_ids=pop_ids,\n",
    "                                                  projections=ns,\n",
    "                                                  polarized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_from_ests = moments.Demographics2D.split_mig(prior_estimated_params, ns,\n",
    "                                                   pop_ids=pop_ids).fold()\n",
    "sfs_split_mig_model = moments.Demographics2D.split_mig(split_mig_params, ns,\n",
    "                                                              pop_ids=pop_ids).fold()\n",
    "sfs_split_no_mig_model = moments.Demographics2D.split_mig(split_no_mig_params, ns,\n",
    "                                                                 pop_ids=pop_ids).fold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.758038564030496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.Inference.ll_multinom(sfs_from_ests, sfs_empirical / np.sum(sfs_empirical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.467354420192853"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.Inference.ll_multinom(sfs_split_mig_model, sfs_empirical / np.sum(sfs_empirical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.534079265957455"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.Inference.ll_multinom(sfs_split_no_mig_model, sfs_empirical / np.sum(sfs_empirical))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benv2",
   "language": "python",
   "name": "benv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
